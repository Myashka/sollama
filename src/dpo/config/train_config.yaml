model:
  name: /raid/models/llama-7b-python_basics_par-fp32 # /raid/models/llama-7b-python_basics-fp32
  load_in_8bit: false
  peft_model_id: null # указать, чтобы загрузить Peft веса с HF
  torch_dtype: fp16 # fp16/null - dtype модели
  device_map: auto # cuda:2
  padding_side: left # try right

lora_config:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  bias: "none"

data:
  dataset_name: "Myashka/SO_Python_basics_QA_human_pref"
  max_prompt_length: 512
  max_answer_length: 256
  use_title: true
  val_size: 500
  use_gen: false
  use_par: true
  use_so: true
  so_fix: 0.15
  num_proc: 8

beta: 0.01
ignore_bias_buffers: false

training_arguments:
  seed: 42
  num_train_epochs: 10
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 4
  learning_rate: 1.0e-05
  optim: "adamw_torch"
  # Oprims: adamw_hf/adamw_torch/adamw_torch_fused/adamw_apex_fused/adamw_anyprecision/adafactor
  weight_decay: 0.1
  adam_beta1: 0.9
  adam_beta2: 0.99
  max_grad_norm: 1

  ### LR SCHEDULER ###
  # TYPES: linear/cosine/cosine_with_restarts/polynomial/constant/constant_with_warmup
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.01

  ### MEMORY OPTIMIZATION ###
  gradient_accumulation_steps: 1 # !
  fp16: true
  gradient_checkpointing: true

  ### EVALUATION ###
  evaluation_strategy: "steps" # steps/epoch
  eval_steps: 0.0125
  # fp16_full_eval: true
  remove_unused_columns: false
  dataloader_drop_last: false
  # predict_with_generate: false

  ### SAVING ###
  save_strategy: "steps" #steps/epoch; if steps needs `save_steps`
  save_steps: 0.0125
  output_dir: /home/st-gorbatovski/sollama/src/dpo/artifacts/experiments/train-dpo-llama_par-7b-bs_4-lr_1.0e5-basics_par-beta_0.1
  save_total_limit: 1
  load_best_model_at_end: true
  resume_from_checkpoint: null

  ### LOGGING CONFIG ###
  logging_strategy: "steps"
  logging_steps: 1
  report_to: 'wandb'
  run_name: train-dpo-llama_par-7b-bs_4-lr_1.0e5-basics_par-beta_0.1