{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st-gorbatovski/.conda/envs/gorbatovski_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/st-gorbatovski/.conda/envs/gorbatovski_env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/st-gorbatovski/.conda/envs/gorbatovski_env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st-gorbatovski/.conda/envs/gorbatovski_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/st-gorbatovski/.conda/envs/gorbatovski_env did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/st-gorbatovski/.conda/envs/gorbatovski_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/var/lib/nvidia-mig-manager/checkpoint.json')}\n",
      "  warn(msg)\n",
      "/home/st-gorbatovski/.conda/envs/gorbatovski_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/st-gorbatovski/.conda/envs/gorbatovski_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/home/st-gorbatovski/.conda/envs/gorbatovski_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import os\n",
    "import addict\n",
    "import torch\n",
    "import peft\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    "    set_seed,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    LlamaConfig\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/st-gorbatovski/sollama/')\n",
    "\n",
    "from src.sft.utils import load_model\n",
    "from src.sft.data import make_inference_dataset\n",
    "from src.sft.models import eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(\n",
    "    dict(\n",
    "        name=\"/raid/models/llama-7b-hf\",\n",
    "        load_in_8bit=False,\n",
    "        peft_model_id=None,\n",
    "        device_map='cuda:0',\n",
    "        torch_dtype=torch.float16,\n",
    "        padding_side='left'\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/st-gorbatovski/.cache/huggingface/datasets/Myashka___csv/Myashka--SO-Python_QA-API_Usage-tanh_score-3d810357b517bfcc/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "Loading cached processed dataset at /home/st-gorbatovski/.cache/huggingface/datasets/Myashka___csv/Myashka--SO-Python_QA-API_Usage-tanh_score-3d810357b517bfcc/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-e5ac4e36ef2353e3.arrow\n"
     ]
    }
   ],
   "source": [
    "test_dataset = make_inference_dataset(tokenizer=tokenizer, **dict(\n",
    "    dataset_name=\"Myashka/SO-Python_QA-API_Usage-tanh_score\",\n",
    "    max_prompt_length=512,\n",
    "    split=\"test\",\n",
    "    use_title=True\n",
    "))\n",
    "dataloader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_config = dict(\n",
    "    do_sample = True,\n",
    "    max_new_tokens = 512,\n",
    "    no_repeat_ngram_size = 2,\n",
    "    top_k = 50,\n",
    "    top_p = 0.9,\n",
    "    use_cache = True,\n",
    "    num_return_sequences = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <unk>\n",
      "1: <s>\n",
      "2: </s>\n",
      "\n",
      "bos token:0\n",
      "pad token:0\n",
      "unk token:0\n",
      "eos token:2\n"
     ]
    }
   ],
   "source": [
    "print(f'0: {tokenizer.decode(0)}')\n",
    "print(f'1: {tokenizer.decode(1)}')\n",
    "print(f'2: {tokenizer.decode(2)}')\n",
    "print()\n",
    "print(f'bos token:{tokenizer.bos_token_id}')\n",
    "print(f'pad token:{tokenizer.pad_token_id}')\n",
    "print(f'unk token:{tokenizer.unk_token_id}')\n",
    "print(f'eos token:{tokenizer.eos_token_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# model.generation_config.eos_token_id = 2\n",
    "print(model.generation_config.bos_token_id)\n",
    "print(model.generation_config.pad_token_id)\n",
    "print(model.generation_config.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(model.generation_config.bos_token_id)\n",
    "print(model.generation_config.pad_token_id)\n",
    "print(model.generation_config.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2000 [00:03<1:53:23,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Title: Nvenc session limit per GPU\\nQuestion: I'm using Imageio, the python library that wraps around ffmpeg to do hardware encoding via nvenc. My issue is that I can't get more than 2 sessions to launch (I am using non-quadro GPUs). Even using multiple GPUs. I looked over NVIDIA's support matrix and they state only 2 sessions per gpu, but it seems to be per system.\\nFor example I have 2 GPUs in a system. I can either use the env variable CUDA_VISIBLE_DEVICES or set the ffmpeg flag -gpu to select the GPU. I've verified gpu usage using Nvidia-smi cli. I can get 2 encoding sessions working on a single gpu. Or 1 session working on 2 separate gpus each. But I can't get 2 encoding sessions working on 2 gpus. \\nEven more strangely if I add more gpus I am still stuck at 2 sessions. I can't launch a third encoding session on a 3rd gpu. I am always stuck at 2 regardless of the # of gpus. Any ideas on how to fix this?\\nAnswer: The limit is set on the system level. It should be set using the nvidia drivers. In the command line, you can see the value by running: nv_set_sessions(0, 0) and that should return the number of sessions allowed per compute device. There is a hard limit of 64.\", \"Title: Nvenc session limit per GPU\\nQuestion: I'm using Imageio, the python library that wraps around ffmpeg to do hardware encoding via nvenc. My issue is that I can't get more than 2 sessions to launch (I am using non-quadro GPUs). Even using multiple GPUs. I looked over NVIDIA's support matrix and they state only 2 sessions per gpu, but it seems to be per system.\\nFor example I have 2 GPUs in a system. I can either use the env variable CUDA_VISIBLE_DEVICES or set the ffmpeg flag -gpu to select the GPU. I've verified gpu usage using Nvidia-smi cli. I can get 2 encoding sessions working on a single gpu. Or 1 session working on 2 separate gpus each. But I can't get 2 encoding sessions working on 2 gpus. \\nEven more strangely if I add more gpus I am still stuck at 2 sessions. I can't launch a third encoding session on a 3rd gpu. I am always stuck at 2 regardless of the # of gpus. Any ideas on how to fix this?\\nAnswer:\\nI haven' t tried it myself but someone posted this fix in the nvidia forum: https://forums.developer.nvidia.com/t/limit-nven-enc-session-to-1-gpux/25840/13. Maybe it works for you as well.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2000 [00:11<3:18:40,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Title: Setup of the Divio CMS Repositories\\nQuestion: The Divio Django CMS offers two servers: TEST and LIVE. Are these also two separate repositories? Or how is this done in the background?\\nI'm wondering because I would have the feeling the LIVE server is its own repository that just pulls from the TEST whenever I press deploy. Is that correct?\\nAnswer: These two repositories are separate and you don't have to change anything in your code to use either of them. The TESTRPO (live) and TEMPRPO will always be available.\\nDivio Labs will be making the repositories more flexible and giving them the same features as Divimasters soon. This means for instance that you'll be able to define the live URL and the staging URL on your own and choose between the two as you see fit.\", \"Title: Setup of the Divio CMS Repositories\\nQuestion: The Divio Django CMS offers two servers: TEST and LIVE. Are these also two separate repositories? Or how is this done in the background?\\nI'm wondering because I would have the feeling the LIVE server is its own repository that just pulls from the TEST whenever I press deploy. Is that correct?\\nAnswer: Thanks for the question.\\nWhen you are using TEAM it is indeed a different repository. However, when you switch to PRO that becomes one repository where the build and the deploy process happens in one go. So if you deploy from TEAMS, the server changes as well. This is done on the same server (the LIVE server) and it only takes a moment. It will not however overwrite the existing TEMPLATE file. Instead it will create a subfolder in TEMAPLTE and then a separate folder for each repository you have. You can see a demo here\\nhttps://demo.divio.com/cms/templates/dev/settings/\\nOn the PRO server you do have multiple repositories so that you can switch from one to the other. But if a deployment is in progress, it waits until it's done before it deploys to LICVE.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2000 [00:19<3:57:22,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Title: How do i retrain the model without losing the earlier model data with new set of data\\nQuestion: for my current requirement, I'm having a dataset of 10k+ faces from 100 different people from which I have trained a model for recognizing the face(s). The model was trained by getting the 128 vectors from the facenet_keras.h5 model and feeding those vector value to the Dense layer for classifying the faces.\\nBut the issue I'm facing currently is\\n\\nif want to train one person face, I have to retrain the whole model once again.\\n\\nHow should I get on with this challenge? I have read about a concept called transfer learning but I have no clues about how to implement it. Please give your suggestion on this issue. What can be the possible solutions to it?\\nAnswer: You should use a pre-trained model that is trained on a much larger dataset than your training set. This way, you only need to feed the pretraind model a new training sample to obtain a high accuracy. If you want, it is possible to fine-tune the network using the image in which you are interested. More about fine tuning can also be found on https://github.com/alexbrodski/face-detection-with-pre-extracted-features/blob/master/models/transfer_model.py#L51-L71\\nReference Link: https:https://towardsdatascience.org/using-a-facenetwork-on-images-and-videos-of-faces-39172b636e11?_branch_match_id=508030977452733588\", \"Title: How do i retrain the model without losing the earlier model data with new set of data\\nQuestion: for my current requirement, I'm having a dataset of 10k+ faces from 100 different people from which I have trained a model for recognizing the face(s). The model was trained by getting the 128 vectors from the facenet_keras.h5 model and feeding those vector value to the Dense layer for classifying the faces.\\nBut the issue I'm facing currently is\\n\\nif want to train one person face, I have to retrain the whole model once again.\\n\\nHow should I get on with this challenge? I have read about a concept called transfer learning but I have no clues about how to implement it. Please give your suggestion on this issue. What can be the possible solutions to it?\\nAnswer: Transfer learning is exactly what you are looking for. I suggest looking at some of the tutorials provided here:\\nhttp://chetan-code.blogspot.com/2016/11/transfer-learning-in-deep-neural-networks.html\\nIf you have questions please don't hesitate to ask.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/2000 [00:23<3:16:41,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Title: How to debug (500) Internal Server Error on Python Waitress server?\\nQuestion: I'm using Python and Flask, served by Waitress, to host a POST API. I'm calling the API from a C# program that posts data and gets a string response. At least 95% of the time, it works fine, but sometimes the C# program reports an error: \\n(500) Internal Server Error.\\nThere is no further description of the error or why it occurs. The only clue is that it usually happens in clusters -- when the error occurs once, it likely occurs several times in a row. Without any intervention, it then goes back to running normally.\\nSince the error is so rare, it is hard to troubleshoot. Any ideas as to how to debug or get more information? Is there error handling I can do from either the C# side or the Flask/Waitress side?\\nAnswer: Python Error: AttributeError: 'str' object has no attribute 'lower'\", \"Title: How to debug (500) Internal Server Error on Python Waitress server?\\nQuestion: I'm using Python and Flask, served by Waitress, to host a POST API. I'm calling the API from a C# program that posts data and gets a string response. At least 95% of the time, it works fine, but sometimes the C# program reports an error: \\n(500) Internal Server Error.\\nThere is no further description of the error or why it occurs. The only clue is that it usually happens in clusters -- when the error occurs once, it likely occurs several times in a row. Without any intervention, it then goes back to running normally.\\nSince the error is so rare, it is hard to troubleshoot. Any ideas as to how to debug or get more information? Is there error handling I can do from either the C# side or the Flask/Waitress side?\\nAnswer: You might use FlushHeadersAndBody to force the browser to send any pending headers and content.  But this sounds like a bug. It might be a problem with the connection timeout that is not being handled. If you are on the same machine, and this error has happened once before, have you tried killing the process? If the issue continues to happen, then you might post a ticket with Waiter and they can help debug it.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/2000 [00:30<3:30:06,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Title: Check inputs in csv file\\nQuestion: I`m new to python. I have a csv file. I need to check whether the inputs are correct or not. The ode should scan through each rows. \\nAll columns for a particular row should contain values of same type: Eg:\\nAll columns of second row should contain only string, \\nAll columns of third row should contain only numbers... etc\\nI tried the following approach, (it may seem blunder):\\nI have only 15 rows, but no idea on number of columns(Its user choice)\\ndf.iloc[1].str.isalpha()\\nThis checks  for string. I don`t know how to check ??\\nAnswer: Hi to go through the csv files, you can use pandas or numpy. They are very fast and easy to handle csv data.\\n\\nIf your csv is of this format. you are having your data like in this dataframe and i am not sure about the number for columns. but you could simply use\\n`df['col1'].isna()`\\nif you want to see if there is a value in column 2 or there isn't. if it's all string values. it will return `True`.\\nfor integers it returns `False`\\nYou can see the details about this method here: https://www.pythonforbeginners.com/pandas/checking-if-the-pandas-dataframe-has-numeric-values-in-a-column\\nHope this helps.\", 'Title: Check inputs in csv file\\nQuestion: I`m new to python. I have a csv file. I need to check whether the inputs are correct or not. The ode should scan through each rows. \\nAll columns for a particular row should contain values of same type: Eg:\\nAll columns of second row should contain only string, \\nAll columns of third row should contain only numbers... etc\\nI tried the following approach, (it may seem blunder):\\nI have only 15 rows, but no idea on number of columns(Its user choice)\\ndf.iloc[1].str.isalpha()\\nThis checks  for string. I don`t know how to check ??\\nAnswer: df= pd.read_csv(\"filename.csv\")\\n   for i in range(len(df)):\\n       if(not(any(bool(j) for j in df[i]))):  <-- check if all the columns have proper input\\n          continue\\n           break\\n\\\\end{code}']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/2000 [00:38<4:14:39,  7.66s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "    output_tokens = model.generate(\n",
    "        input_ids=batch[\"input_ids\"].to(model.device),\n",
    "        attention_mask=batch[\"attention_mask\"].to(model.device),\n",
    "        **generate_config).cpu().numpy()\n",
    "    if i == 5:\n",
    "        break\n",
    "    \n",
    "    print(tokenizer.batch_decode(output_tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: how do I upgrade pip on Mac?\n",
      "Question: I cannot upgrade pip on my Mac from the Terminal. \n",
      "According to the documentation I have to type the command:\n",
      "pip install -U pip\n",
      "I get the error message in the Terminal:\n",
      "pip: command not found\n",
      "I have Mac OS 10.14.2, python 3.7.2 and pip 18.1.\n",
      "I want to upgrade to pip 19.2.3\n",
      "Answer: MacOS doesn't provide pip by default, so you will have use a third party source. For that purpose we recommend pypi, which provides the latest versions of pip (but requires you to download them). For instructions, please visit:  https://stackoverflow.com/questions/33603583/how-do-i-install-pip-on-mac-os-x\n",
      "How to enable \"Bot\" to my Facebook Business Page?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: how do I upgrade pip on Mac?\n",
      "Question: I cannot upgrade pip on my Mac from the Terminal. \n",
      "According to the documentation I have to type the command:\n",
      "pip install -U pip\n",
      "I get the error message in the Terminal:\n",
      "pip: command not found\n",
      "I have Mac OS 10.14.2, python 3.7.2 and pip 18.1.\n",
      "I want to upgrade to pip 19.2.3\n",
      "Answer: If you are running MacOS and have upgraded your system, then you may need to do a different command. You can go to Mac App Store and get pip by searching there. Once you have the pip app, just search for \"pip\" and hit install to get it. Make sure you install pip using the app from MacAppStore, or you will not be able to run it in a terminal. Then, to reinstall pip, go back to terminal and run \"python\" to bring it up. then do \"which pip\" then \"export PIP_PATH=PATH\". If pip is not working, you should try again. If it is still not happening, try \"sudo\" in front of your commands, because some apps don't work on the standard terminal commands. It should work. if it doesn'ts then there is a problem with you system. good luck.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gorbatovski_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
