{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics transformers datasets wandb accelerate nltk sentencepiece bitsandbytes scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "from torchmetrics import SacreBLEUScore\n",
    "import nltk\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from transformers import set_seed, LlamaForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS token: </s> EOS toke id: 2\n",
      "PAD token: <unk> PAD toke id: 0\n",
      "UNK token:  UNK toke id: 0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    \"decapoda-research/llama-7b-hf\", eos_token=\"</s>\"\n",
    ")\n",
    "tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
    "tokenizer.padding_side = \"left\"\n",
    "print(f\"EOS token: {tokenizer.eos_token} EOS toke id: {tokenizer.eos_token_id}\")\n",
    "print(f\"PAD token: {tokenizer.pad_token} PAD toke id: {tokenizer.pad_token_id}\")\n",
    "print(f\"UNK token: {tokenizer.unk_token} UNK toke id: {tokenizer.unk_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:10<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"decapoda-research/llama-7b-hf\",\n",
    "    load_in_8bit=True,\n",
    "    # torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inference(\n",
    "    dataset_name,\n",
    "    tokenizer,\n",
    "    split,\n",
    "    max_prompt_length,\n",
    "    padding=\"longest\",\n",
    "    truncate_promt=True,\n",
    "):\n",
    "    def _prepare_prompt(question):\n",
    "        return f\"Question: {question}\\nAnswer:\"\n",
    "\n",
    "    def promt_tokenize(examples):\n",
    "        if truncate_promt:\n",
    "            q_toks = tokenizer.encode(examples[\"Question\"])\n",
    "            q_toks = q_toks[: max_prompt_length - 8]\n",
    "            tmp = tokenizer.decode(q_toks).strip()\n",
    "        else:\n",
    "            tmp = examples[\"Question\"]\n",
    "\n",
    "        tmp = _prepare_prompt(tmp)\n",
    "\n",
    "        tokenized_dict = tokenizer(\n",
    "            tmp, padding=padding, max_length=max_prompt_length, truncation=True\n",
    "        )\n",
    "\n",
    "        return tokenized_dict\n",
    "\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    dataset = dataset.map(promt_tokenize)\n",
    "    dataset.set_format(\n",
    "        type=\"torch\", columns=[\"Question\", \"Answer\", \"input_ids\", \"attention_mask\"]\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = dict(dataset_name = 'Myashka/SO-Python_QA-API_Usage-tanh_score',\n",
    "                   split = 'test',\n",
    "                   max_prompt_length=512,\n",
    "                   padding='longest',\n",
    "                   truncate_promt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/root/.cache/huggingface/datasets/Myashka___csv/Myashka--SO-Python_QA-API_Usage-tanh_score-55f862e88e900e46/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/Myashka___csv/Myashka--SO-Python_QA-API_Usage-tanh_score-55f862e88e900e46/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b53552a141b1d64b.arrow\n"
     ]
    }
   ],
   "source": [
    "test_dataset = prepare_inference(tokenizer=tokenizer, **data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset['input_ids'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_config = dict(\n",
    "    do_sample=True,\n",
    "    max_new_tokens=512,\n",
    "    no_repeat_ngram_size=2,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    # min_new_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = test_dataset.remove_columns(['Q_CreationDate', 'Title', 'Question', 'Answer', 'Score', 'Is_accepted', 'N_answers', 'Q_Id'])\n",
    "test_dataset = test_dataset.remove_columns(['Q_CreationDate', 'Title','input_ids', 'attention_mask', 'Score', 'Is_accepted', 'N_answers', 'Q_Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "input_dataloader = DataLoader(input_dataset, batch_size=1, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, input_dataloader = accelerator.prepare(model, input_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(input_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens = model.generate(**batch,\n",
    "                                    **generate_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: <unk>I'm using Imageio, the python library that wraps around ffmpeg to do hardware encoding via nvenc. My issue is that I can't get more than 2 sessions to launch (I am using non-quadro GPUs). Even using multiple GPUs. I looked over NVIDIA's support matrix and they state only 2 sessions per gpu, but it seems to be per system.\n",
      "For example I have 2 GPUs in a system. I can either use the env variable CUDA_VISIBLE_DEVICES or set the ffmpeg flag -gpu to select the GPU. I've verified gpu usage using Nvidia-smi cli. I can get 2 encoding sessions working on a single gpu. Or 1 session working on 2 separate gpus each. But I can't get 2 encoding sessions working on 2 gpus. \n",
      "Even more strangely if I add more gpus I am still stuck at 2 sessions. I can't launch a third encoding session on a 3rd gpu. I am always stuck at 2 regardless of the # of gpus. Any ideas on how to fix this?\n",
      "Answer: There are a few known issues with ImageIO/nvencc that are reported against the ImageIo/FFmpeg project. These issues do relate to more that just GPU parallelization. One is with using 8bit YUV input and the other is related to not allowing the 4th argument to set. The problem with GPU encoding is actually reported separately, and there're a couple of open issues for that one in the FFmpeg issue tracker. https://github.com/opencv/libimgcodecs/issues/1209 and httpshttps://www.ffmpeg.org/trac/ffplay/browser/trunk/enc/vpxenc.c?r=3822#L176\n",
      "Comment: I’m facing the same problem here. If I use -profile high -parallel_encoding 0 and -threads 64 my CPU goes crazy (it gets about 55% usage). But with -paral_encoding 96 it works perfectly!\n",
      "\n",
      "The problem seems similar to the one described here:\n",
      "https: //github .com / opencv / libimg codecs / issues / 703\n",
      "(which is fixed in ffplay).\n",
      "Also in my case it happens only on Ubuntu ≥  (14.04,  xenial). There I had to install FFMpeg separately (with Ubuntu package manager) in order to get it working. Otherwise it’s not recognized. For some reason Ubuntu installs ffmeg on /usr/local instead of /opt/bin/ where it is installed by default in Linux Mint. Also the -v version of ffmpeg works for me, while ffprobe is not used at all (so, only one process). So the main problem is probably with Ubuntu’ install location of FFMPEG. It may be something related with it.\n"
     ]
    }
   ],
   "source": [
    "print(*tokenizer.batch_decode(output_tokens, skip_special_tokens=True), sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = ROUGEScore()\n",
    "bleu = SacreBLEUScore(1, lowercase=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
