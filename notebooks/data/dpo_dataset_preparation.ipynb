{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets.combine import concatenate_datasets\n",
    "\n",
    "\n",
    "def print_statistics(dataset, name=\"Dataset\"):\n",
    "    \"\"\"Print statistics about the dataset.\"\"\"\n",
    "    total = len(dataset)\n",
    "\n",
    "    gen_k_count = par_k_only_count = par_j_only_count = par_both_count = so_k_count = 0\n",
    "\n",
    "    for x in dataset:\n",
    "        if x[\"is_gen_k\"]:\n",
    "            gen_k_count += 1\n",
    "        if x[\"is_par_k\"] and not x[\"is_par_j\"]:\n",
    "            par_k_only_count += 1\n",
    "        if not x[\"is_par_k\"] and x[\"is_par_j\"]:\n",
    "            par_j_only_count += 1\n",
    "        if x[\"is_par_k\"] and x[\"is_par_j\"]:\n",
    "            par_both_count += 1\n",
    "        if not (x[\"is_gen_k\"] or x[\"is_par_k\"]):\n",
    "            so_k_count += 1\n",
    "\n",
    "    par_count = par_k_only_count + par_j_only_count + par_both_count\n",
    "    so_j_count = total - par_j_only_count - par_both_count\n",
    "\n",
    "    print(f\"Statistics for {name}:\")\n",
    "    print(f\"Total: {total}\")\n",
    "    print(f\"Generated (k): {gen_k_count} ({100 * gen_k_count / total:.2f}%)\")\n",
    "    print(\n",
    "        f\"Paraphrased (k only): {par_k_only_count} ({100 * par_k_only_count / total:.2f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Paraphrased (j only): {par_j_only_count} ({100 * par_j_only_count / total:.2f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Paraphrased (both j & k): {par_both_count} ({100 * par_both_count / total:.2f}%)\"\n",
    "    )\n",
    "    print(f\"SO (k): {so_k_count} ({100 * so_k_count / total:.2f}%)\")\n",
    "    print(f\"SO (j): {so_j_count} ({100 * so_j_count / total:.2f}%)\")\n",
    "    print(f\"Total Paraphrased: {par_count} ({100 * par_count / total:.2f}%)\")\n",
    "    print(\n",
    "        f\"Total SO: {total - gen_k_count - par_count} ({100 * (total - gen_k_count - par_count) / total:.2f}%)\"\n",
    "    )\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def make_datasets(\n",
    "    dataset_name,\n",
    "    tokenizer,\n",
    "    max_prompt_length,\n",
    "    max_answer_length,\n",
    "    use_title=False,\n",
    "    test_size=1000,\n",
    "    val_size=500,\n",
    "    use_gen=False,\n",
    "    use_par=False,\n",
    "    use_so=True,\n",
    "    do_train=True,\n",
    "    num_proc=8,\n",
    "    so_fix=None,\n",
    "):\n",
    "    def _prepare_prompt(question, title=None):\n",
    "        if title:\n",
    "            return f\"Title: {title}\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "        return f\"Question: {question}\\n\\nAnswer:\"\n",
    "    \n",
    "    def truncate_question(question, title, max_length):\n",
    "        prompt_skeleton = \"Title: {}\\nQuestion: {}\\n\\nAnswer:\" if title else \"Question: {}\\n\\nAnswer:\"\n",
    "        prompt_length = len(prompt_skeleton.format(title if title else \"\", \"\", \"\"))\n",
    "        truncated_question = question[:max_length - prompt_length]\n",
    "        return truncated_question\n",
    "\n",
    "    def prompt_tokenize(example):\n",
    "        question = example[\"Question\"]\n",
    "        title = example[\"Title\"] if use_title else None\n",
    "        truncated_question = truncate_question(question, title, max_prompt_length)\n",
    "\n",
    "        chosen = example[\"response_j\"]\n",
    "        rejected = example[\"response_k\"]\n",
    "\n",
    "        chosen_encoded = tokenizer.encode(chosen, max_length=max_answer_length, truncation=True)\n",
    "        rejected_encoded = tokenizer.encode(rejected, max_length=max_answer_length, truncation=True)\n",
    "\n",
    "        chosen_text = tokenizer.decode(chosen_encoded, skip_special_tokens=True)\n",
    "        rejected_text = tokenizer.decode(rejected_encoded, skip_special_tokens=True)\n",
    "\n",
    "        prompt = _prepare_prompt(truncated_question, title)\n",
    "        return {\"prompt\": prompt, \"chosen\": chosen_text, \"rejected\": rejected_text}\n",
    "\n",
    "    def filter_datasets(example):\n",
    "        if use_gen and (example[\"is_gen_k\"] or example[\"is_gen_j\"]):\n",
    "            return True\n",
    "        if use_par and (example[\"is_par_k\"] or example[\"is_par_j\"]):\n",
    "            return True\n",
    "        if use_so and not (example[\"is_gen_k\"] or example[\"is_gen_j\"] or example[\"is_par_k\"] or example[\"is_par_j\"]):\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    if not use_gen and not use_par:\n",
    "        so_fix = None\n",
    "\n",
    "    dataset = load_dataset(dataset_name)[\"train\"]\n",
    "    dataset = load_dataset(dataset_name)[\"train\"].filter(filter_datasets, num_proc=num_proc)\n",
    "    dataset = dataset.map(prompt_tokenize, num_proc=num_proc)\n",
    "\n",
    "    if not so_fix:\n",
    "        train_test_split = dataset.train_test_split(test_size=test_size)\n",
    "        temp_train_data = train_test_split[\"train\"]\n",
    "        test_data = train_test_split[\"test\"]\n",
    "\n",
    "        train_val_split = temp_train_data.train_test_split(test_size=val_size)\n",
    "        train_data = train_val_split[\"train\"]\n",
    "        val_data = train_val_split[\"test\"]\n",
    "\n",
    "        if do_train:\n",
    "            print_statistics(train_data, \"Train Dataset\")\n",
    "            print_statistics(val_data, \"Validation Dataset\")\n",
    "\n",
    "            col_to_remove = [col for col in train_data.column_names if col not in ['prompt', 'chosen', 'rejected']]\n",
    "            train_data = train_data.remove_columns(col_to_remove)\n",
    "            val_data = val_data.remove_columns(col_to_remove)\n",
    "\n",
    "            return train_data, val_data\n",
    "\n",
    "        print_statistics(test_data, \"Test Dataset\")\n",
    "        col_to_remove = [col for col in test_data.column_names if col not in ['prompt', 'chosen', 'rejected']]\n",
    "        test_data = test_data.remove_columns(col_to_remove)\n",
    "        return test_data\n",
    "    else:\n",
    "        test_so_part = int(test_size * so_fix)\n",
    "        test_any_part = test_size - test_so_part\n",
    "\n",
    "        val_so_part = int(val_size * so_fix)\n",
    "        val_any_part = val_size - val_so_part\n",
    "\n",
    "        any_dataset = dataset.filter(\n",
    "            lambda x: (\n",
    "                x[\"is_gen_k\"] or x[\"is_gen_j\"] or x[\"is_par_k\"] or x[\"is_par_j\"]\n",
    "            ),\n",
    "            num_proc=num_proc,\n",
    "        )\n",
    "        so_dataset = dataset.filter(\n",
    "            lambda x: not (\n",
    "                x[\"is_gen_k\"] or x[\"is_gen_j\"] or x[\"is_par_k\"] or x[\"is_par_j\"]\n",
    "            ),\n",
    "            num_proc=num_proc,\n",
    "        )\n",
    "\n",
    "        test_so_split = so_dataset.train_test_split(test_size=test_so_part)\n",
    "        test_any_split = any_dataset.train_test_split(test_size=test_any_part)\n",
    "\n",
    "        if not do_train:\n",
    "            test_data = concatenate_datasets(\n",
    "                [test_so_split[\"test\"], test_any_split[\"test\"]]\n",
    "            )\n",
    "            print_statistics(test_data, \"Test Dataset\")\n",
    "            col_to_remove = [col for col in test_data.column_names if col not in ['prompt', 'chosen', 'rejected']]\n",
    "            test_data = test_data.remove_columns(col_to_remove)\n",
    "            return test_data\n",
    "\n",
    "        train_val_so_split = test_so_split[\"train\"].train_test_split(\n",
    "            test_size=val_so_part\n",
    "        )\n",
    "        train_val_any_split = test_any_split[\"train\"].train_test_split(\n",
    "            test_size=val_any_part\n",
    "        )\n",
    "\n",
    "        train_data = concatenate_datasets(\n",
    "            [train_val_so_split[\"train\"], train_val_any_split[\"train\"]]\n",
    "        )\n",
    "        val_data = concatenate_datasets(\n",
    "            [train_val_so_split[\"test\"], train_val_any_split[\"test\"]]\n",
    "        )\n",
    "\n",
    "        print_statistics(train_data, \"Train Dataset\")\n",
    "        print_statistics(val_data, \"Validation Dataset\")\n",
    "\n",
    "        col_to_remove = [col for col in train_data.column_names if col not in ['prompt', 'chosen', 'rejected']]\n",
    "        train_data = train_data.remove_columns(col_to_remove)\n",
    "        val_data = val_data.remove_columns(col_to_remove)\n",
    "\n",
    "        return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"/raid/models/llama-7b-python_basics-fp32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/st-gorbatovski/.cache/huggingface/datasets/Myashka___json/Myashka--SO_Python_basics_QA_human_pref-0870bdfdc441eea5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|██████████| 1/1 [00:00<00:00, 258.78it/s]\n",
      "Loading cached processed dataset at /home/st-gorbatovski/.cache/huggingface/datasets/Myashka___json/Myashka--SO_Python_basics_QA_human_pref-0870bdfdc441eea5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ae0fa9789c363fd5_*_of_00008.arrow\n",
      "Loading cached processed dataset at /home/st-gorbatovski/.cache/huggingface/datasets/Myashka___json/Myashka--SO_Python_basics_QA_human_pref-0870bdfdc441eea5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-399abe4ecbd26e12_*_of_00008.arrow\n",
      "Loading cached processed dataset at /home/st-gorbatovski/.cache/huggingface/datasets/Myashka___json/Myashka--SO_Python_basics_QA_human_pref-0870bdfdc441eea5/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8879a417724edf1f_*_of_00008.arrow\n",
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Train Dataset:\n",
      "Total: 96526\n",
      "Generated (k): 0 (0.00%)\n",
      "Paraphrased (k only): 35857 (37.15%)\n",
      "Paraphrased (j only): 0 (0.00%)\n",
      "Paraphrased (both j & k): 54728 (56.70%)\n",
      "SO (k): 5941 (6.15%)\n",
      "SO (j): 41798 (43.30%)\n",
      "Total Paraphrased: 90585 (93.85%)\n",
      "Total SO: 5941 (6.15%)\n",
      "--------------------------------------------------\n",
      "Statistics for Validation Dataset:\n",
      "Total: 500\n",
      "Generated (k): 0 (0.00%)\n",
      "Paraphrased (k only): 179 (35.80%)\n",
      "Paraphrased (j only): 0 (0.00%)\n",
      "Paraphrased (both j & k): 246 (49.20%)\n",
      "SO (k): 75 (15.00%)\n",
      "SO (j): 254 (50.80%)\n",
      "Total Paraphrased: 425 (85.00%)\n",
      "Total SO: 75 (15.00%)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train, val = make_datasets(\"Myashka/SO_Python_basics_QA_human_pref\", tokenizer, 512, 256, True, so_fix=0.15, use_par=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gorbatovski_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
